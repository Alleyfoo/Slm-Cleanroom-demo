================================================================================
REPOSITORY EXPORT
================================================================================

Repository Path: C:\Users\pertt\slm-cleanroom-demo

--------------------------------------------------------------------------------

================================================================================
FILE: README.md
================================================================================

# slm-Cleanroom

Local product description cleaner pipeline.

> **Design Document:** see [docs/design_document.md](docs/design_document.md).  
> All new changes must reference the sections they touch in the Design Document.

## Getting started

### Quickstart (Codespace/local)
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# choose a model (free, small):
export HF_REPO_ID="bartowski/TinyLlama-1.1B-1T-GGUF"
export HF_FILENAME="TinyLlama-1.1B-1T-instruct.Q4_K_M.gguf"

# download only if missing ‚Üí models/<file>.gguf
python -m app.model_download

# set path for runtime
export MODEL_PATH="$PWD/models/$HF_FILENAME"
# optionally raise context window (only if the model supports it)
export CTX=4096
```

Install dependencies and run the API server:

```bash
uvicorn app.server:app --host 0.0.0.0 --port 8000 --reload
```

CLI example:

```bash
python -m cli.clean_file input.txt -o output.json
```

### Optional Finnish spellcheck (Voikko)
On Ubuntu/Debian:
```bash
sudo apt-get update
sudo apt-get install -y python3-libvoikko voikko-fi
```

No code changes required; the pipeline auto-enables Voikko if available.

### Batch run (CSV ‚Üí CSV)

```bash
python cli/clean_table.py data/mock_inputs.csv -o data/mock_outputs.csv \
  --model-path "$PWD/models/$HF_FILENAME" --workers 4
```

### Streamlit review UI

```bash
streamlit run ui/app.py
```

### Docker
Build an image from the provided Dockerfile:
```bash
docker build -t slm-cleanroom .
```

### Contributing
Before proposing changes, update or reference the relevant sections in [docs/design_document.md](docs/design_document.md). PRs without a Design Document reference may be rejected.

Run the batch cleaner with a bind-mounted model:
```bash
docker run --rm -v $(pwd)/models:/models -v $(pwd):/app -e MODEL_PATH=/models/<file>.gguf \
  -w /app python:3.12-slim bash -lc "pip install -r requirements.txt && python cli/clean_table.py data/mock_inputs.csv -o /app/out.csv"
```

### Benchmarking

Run a quick performance benchmark on a sample of rows to guide model selection:

```bash
python tools/bench.py --file data/mock_inputs.csv --workers 2 --samples 200
```

The script reports median and 95p latency per row, throughput, JSON retry rate and flag distribution.



--------------------------------------------------------------------------------

================================================================================
FILE: repo_export.txt
================================================================================



--------------------------------------------------------------------------------

================================================================================
FILE: requirements.txt
================================================================================

fastapi
uvicorn
langid
click
pandas
openpyxl
huggingface_hub
pytest
pytest-mock
loguru
rapidfuzz


--------------------------------------------------------------------------------

================================================================================
FILE: run_demo.sh
================================================================================

#!/bin/bash
echo "Installing dependencies..."
pip install -r requirements.txt

echo "Downloading stub model (if not present)..."
python -m app.model_download

echo "Running batch clean..."
python cli/clean_table.py data/mock_inputs.csv -o data/mock_outputs.csv --model-path "$PWD/models/TinyLlama-1.1B-1T-instruct.Q4_K_M.gguf" --workers 2

echo "Results:"
cat data/mock_outputs.csv


--------------------------------------------------------------------------------

================================================================================
FILE: .devcontainer\devcontainer.json
================================================================================

{
  "name": "slm-Cleanroom",
  "build": {
    "context": "..",
    "dockerfile": "Dockerfile"
  }
}


--------------------------------------------------------------------------------

================================================================================
FILE: .github\pull_request_template.md
================================================================================

## Summary
(What this PR changes; keep it concise.)

## Design Document Reference
- Section(s) touched in `docs/design_document.md`:
  - e.g., ‚Äú2. Functional Requirements‚Äù, ‚Äú6. Guardrails & Validation‚Äù

## Checklists
- [ ] I updated or reviewed the relevant sections in the Design Document.
- [ ] The change preserves TERM & numeric invariants.
- [ ] JSON output is valid and covered by tests.
- [ ] (If batch) I ran `cli/clean_table.py` on `data/mock_inputs.csv`.

## Testing
- Commands and outputs:

examples here



--------------------------------------------------------------------------------

================================================================================
FILE: .github\workflows\pr-dd-check.yml
================================================================================

name: PR requires Design Document reference
on:
  pull_request:
    types: [opened, edited, synchronize]
jobs:
  dd-check:
    runs-on: ubuntu-latest
    steps:
      - name: Check PR body mentions Design Document
        env:
          PR_BODY: ${{ github.event.pull_request.body }}
        run: |
          echo "PR body:"
          echo "$PR_BODY"
          if ! echo "$PR_BODY" | grep -qi "design document"; then
            echo "::error::PR description must mention the Design Document (link or section)."
            exit 1
          fi


--------------------------------------------------------------------------------

================================================================================
FILE: .github\workflows\test.yml
================================================================================

name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-mock
      - name: Run tests
        run: pytest -v
        env:
          MODEL_PATH: "dummy"


--------------------------------------------------------------------------------

================================================================================
FILE: app\checkpointing.py
================================================================================

import csv
from pathlib import Path
from typing import Dict, Iterable, Set


class Checkpointer:
    """CSV-based checkpointing for idempotent batch processing."""

    def __init__(self, output_path: Path, error_path: Path, id_field: str = "id", fieldnames: Iterable[str] = ()):
        self.output_path = output_path
        self.error_path = error_path
        self.id_field = id_field
        self.fieldnames = list(fieldnames)
        self._processed: Set[str] = set()
        self._load_processed()

    def _load_processed(self) -> None:
        if not self.output_path.exists():
            return
        with self.output_path.open("r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if self.id_field in row and row[self.id_field]:
                    self._processed.add(str(row[self.id_field]))

    def is_processed(self, row_id: str) -> bool:
        return str(row_id) in self._processed

    def append_row(self, row: Dict) -> None:
        is_new = not self.output_path.exists()
        with self.output_path.open("a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=self.fieldnames or row.keys())
            if is_new:
                writer.writeheader()
            writer.writerow(row)
        if self.id_field in row:
            self._processed.add(str(row[self.id_field]))

    def append_error(self, row_id: str, error: str, text: str) -> None:
        is_new = not self.error_path.exists()
        with self.error_path.open("a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=[self.id_field, "error", "text"])
            if is_new:
                writer.writeheader()
            writer.writerow({self.id_field: row_id, "error": error, "text": text})


--------------------------------------------------------------------------------

================================================================================
FILE: app\config.py
================================================================================

import os


MODEL_PATH = os.environ.get('MODEL_PATH')
N_THREADS = int(os.environ.get('N_THREADS', '8'))
CTX = int(os.environ.get('CTX', '2048'))
TEMP = float(os.environ.get('TEMP', '0.0'))
MAX_TOKENS = int(os.environ.get('MAX_TOKENS', '512'))


--------------------------------------------------------------------------------

================================================================================
FILE: app\entity_lock.py
================================================================================

import re
from typing import Dict, List, Tuple


ENTITY_PATTERNS: List[Tuple[str, re.Pattern]] = [
    ("price", re.compile(r"\b\d+(?:[.,]\d+)?\s?(?:‚Ç¨|eur|e)\b", flags=re.IGNORECASE)),
    ("sku", re.compile(r"\b[A-Z0-9]{2,}-[A-Z0-9\-]{2,}\b", flags=re.IGNORECASE)),
    ("size", re.compile(r"\b(?:size\s+)?(?:XS|S|M|L|XL|XXL)\b", flags=re.IGNORECASE)),
    ("dimensions", re.compile(r"\b\d+(?:[x√ó]\d+){1,2}\s?(?:mm|cm)?\b", flags=re.IGNORECASE)),
]


def extract_entities(text: str) -> List[Dict]:
    """Return list of entity locks with spans and values."""
    locks: List[Dict] = []
    for ent_type, pattern in ENTITY_PATTERNS:
        for match in pattern.finditer(text):
            locks.append(
                {
                    "type": ent_type,
                    "value": match.group(0),
                    "span": [match.start(), match.end()],
                }
            )
    return locks


def merge_protected_terms(terms: List[str], locks: List[Dict]) -> List[str]:
    """Combine user-provided protected terms with extracted entity values."""
    merged = list(terms)
    for lock in locks:
        if lock["value"] not in merged:
            merged.append(lock["value"])
    return merged


def enforce_entity_lock(original: str, clean_text: str, locks: List[Dict]) -> Tuple[str, List[Dict]]:
    """Ensure locked entities survive; revert and flag if they don't."""
    flags: List[Dict] = []
    final_text = clean_text
    for lock in locks:
        if lock["value"] not in final_text:
            # Revert to original to avoid data corruption and flag for review.
            final_text = original
            flags.append(
                {
                    "type": "locked_entity_changed",
                    "entity_type": lock["type"],
                    "value": lock["value"],
                }
            )
            break
    return final_text, flags


--------------------------------------------------------------------------------

================================================================================
FILE: app\guardrails.py
================================================================================

import json
import re
from typing import Dict, List

SCHEMA_KEYS = {"clean_text", "flags", "changes"}
JSON_START = "<JSON>"
JSON_END = "</JSON>"


def _coerce_payload(raw: str) -> Dict:
    """Load JSON and enforce minimal schema defaults."""
    obj = json.loads(raw)
    if not isinstance(obj, dict):
        raise ValueError("JSON schema mismatch")
    obj.setdefault("clean_text", "")
    obj.setdefault("flags", [])
    obj.setdefault("changes", [])
    for key in ("flags", "changes"):
        if not isinstance(obj.get(key, []), list):
            raise ValueError(f"{key} must be a list")
    obj["clean_text"] = str(obj.get("clean_text", ""))
    return obj


def extract_json(text: str, start: str = JSON_START, end: str = JSON_END) -> Dict:
    """Extract a JSON object, preferring sentinel markers when present."""
    if start in text and end in text:
        i = text.find(start)
        j = text.rfind(end)
        if i != -1 and j != -1 and i < j:
            raw = text[i + len(start) : j].strip()
            obj = _coerce_payload(raw)
            validate_json_schema(obj)
            return obj

    i, j = text.find("{"), text.rfind("}")
    if i == -1 or j == -1 or i > j:
        raise ValueError("No JSON object found")
    obj = _coerce_payload(text[i : j + 1])
    validate_json_schema(obj)
    return obj


def validate_json_schema(obj: Dict) -> None:
    """Validate that *obj* matches the minimal result schema."""
    if not isinstance(obj, dict) or not SCHEMA_KEYS.issubset(obj.keys()):
        raise ValueError("JSON schema mismatch")
    for key in ("flags", "changes"):
        if not isinstance(obj.get(key, []), list):
            raise ValueError(f"{key} must be a list")
    obj.setdefault("clean_text", "")


def forbid_changes_in_terms(original: str, clean_text: str) -> None:
    pattern = re.compile(r"<TERM>(.*?)</TERM>")
    if pattern.findall(original) != pattern.findall(clean_text):
        raise ValueError("TERM content changed")


def post_validate(original: str, result: Dict) -> List[str]:
    flags: List[str] = []
    orig_nums = re.findall(r"\d+", original)
    new_nums = re.findall(r"\d+", result.get("clean_text", ""))
    if orig_nums != new_nums:
        flags.append("numeric_change")
    return flags


--------------------------------------------------------------------------------

================================================================================
FILE: app\io_utils.py
================================================================================

from pathlib import Path
import pandas as pd
import json
from typing import List, Dict, Any

def read_table(path: str) -> pd.DataFrame:
    p = Path(path)
    if p.suffix.lower() in {".xlsx", ".xls"}:
        return pd.read_excel(p)
    return pd.read_csv(p)

def write_table(df: pd.DataFrame, path: str) -> None:
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() in {".xlsx", ".xls"}:
        df.to_excel(p, index=False)
    else:
        df.to_csv(p, index=False)

def parse_terms(x: Any) -> List[str]:
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return []
    if isinstance(x, list):
        return [str(t).strip() for t in x]
    # "term1; term2; term3"
    return [t.strip() for t in str(x).split(";") if t.strip()]

def serialize(obj: Any) -> str:
    return json.dumps(obj, ensure_ascii=False)


--------------------------------------------------------------------------------

================================================================================
FILE: app\lang_utils.py
================================================================================

import re
from typing import List, Dict

try:  # pragma: no cover - optional dependency
    import langid  # type: ignore
except Exception:  # pragma: no cover
    langid = None


def segment_sentences(text: str) -> List[Dict]:
    pattern = re.compile(r'[^.!?]+[.!?]*', re.MULTILINE)
    segments: List[Dict] = []
    for match in pattern.finditer(text):
        segments.append({'start': match.start(), 'end': match.end(), 'text': text[match.start():match.end()]})
    return segments


def detect_lang(text: str) -> str:
    if langid:
        lang, _ = langid.classify(text)
        return lang
    if re.search(r'[√•√§√∂√Ö√Ñ√ñ]', text):
        return 'fi'
    fi_words = {'on', 'ja', 't√§m√§', 'hyv√§', 'takki', 'kaupungilla', 'suosittu', 'malli', 'klassikko'}
    tokens = re.findall(r'\w+', text.lower())
    if any(t in fi_words for t in tokens):
        return 'fi'
    return 'en'


def lang_spans(text: str) -> List[Dict]:
    spans: List[Dict] = []
    for match in re.finditer(r'\b\w+\b', text, flags=re.UNICODE):
        token = match.group(0)
        lang = detect_lang(token)
        spans.append({'start': match.start(), 'end': match.end(), 'lang': lang, 'text': token})
    return spans


def mask_terms(text: str, terms: List[str]) -> str:
    if not terms:
        return text
    for term in terms:
        text = re.sub(re.escape(term), lambda m: f"<TERM>{m.group(0)}</TERM>", text)
    return text


--------------------------------------------------------------------------------

================================================================================
FILE: app\logging_utils.py
================================================================================

import sys
import uuid
from typing import Optional, Tuple

from loguru import logger

# Configure a single JSON logger sink
logger.remove()
logger.add(sys.stdout, serialize=True, backtrace=False, diagnose=False)


def get_logger(correlation_id: Optional[str] = None) -> Tuple["logger", str]:
    """Return a logger bound with a correlation id."""
    cid = correlation_id or str(uuid.uuid4())
    return logger.bind(correlation_id=cid), cid


--------------------------------------------------------------------------------

================================================================================
FILE: app\model_download.py
================================================================================

from pathlib import Path
import os, shutil
from huggingface_hub import hf_hub_download


def _validate_filename(fn: str):
    ok = (".gguf", ".ggml", ".bin")
    if not fn.lower().endswith(ok):
        raise ValueError(
            f"HF_FILENAME must be a model artifact (e.g. .gguf). Got: {fn!r}"
        )

DEFAULT_REPO_ID = os.environ.get("HF_REPO_ID", "bartowski/TinyLlama-1.1B-1T-GGUF")
DEFAULT_FILENAME = os.environ.get("HF_FILENAME", "TinyLlama-1.1B-1T-instruct.Q4_K_M.gguf")
DEFAULT_DIR = os.environ.get("MODELS_DIR", "models")

def ensure_model(repo_id: str = DEFAULT_REPO_ID, filename: str = DEFAULT_FILENAME, models_dir: str = DEFAULT_DIR) -> str:
    _validate_filename(filename)
    models = Path(models_dir)
    models.mkdir(parents=True, exist_ok=True)
    dest = models / filename
    if dest.exists():
        return str(dest)
    tmp = hf_hub_download(repo_id=repo_id, filename=filename, local_dir=".")  # ladattu /tmp/.cache ‚Üí kopio
    shutil.copy2(tmp, dest)
    return str(dest)

if __name__ == "__main__":
    path = ensure_model()
    print("Model ready at:", path)


--------------------------------------------------------------------------------

================================================================================
FILE: app\pipeline.py
================================================================================

from typing import List, Dict, Optional
import difflib
import json
import re

from .lang_utils import mask_terms, lang_spans
from .slm_llamacpp import slm_cleanup as _slm_cleanup

from .guardrails import (
    validate_json_schema,
    forbid_changes_in_terms,
    post_validate,
    extract_json,
)
from .entity_lock import extract_entities, enforce_entity_lock
from .logging_utils import get_logger

from .config import MODEL_PATH, N_THREADS, CTX, TEMP, MAX_TOKENS

try:  # optional dependency
    from llama_cpp import Llama  # type: ignore
except Exception:  # pragma: no cover - llama_cpp is optional
    Llama = None  # type: ignore

_LLAMA = None


def _load_llama():
    """Lazily load llama-cpp model using environment configuration."""
    global _LLAMA
    if _LLAMA is None and Llama is not None and MODEL_PATH:
        try:  # pragma: no cover - exercised only when llama_cpp is installed
            _LLAMA = Llama(
                model_path=MODEL_PATH,
                n_threads=N_THREADS,
                n_ctx=CTX,
            )
        except Exception:
            _LLAMA = None
    return _LLAMA

try:
    from spellchecker import SpellChecker
    SP_EN = SpellChecker(language="en")
except Exception:
    SP_EN = None

# Optional Voikko (FI)
_VOIKKO = None
try:
    import libvoikko
    _VOIKKO = libvoikko.Voikko("fi")
except Exception:
    _VOIKKO = None

try:
    from rapidfuzz import fuzz
except Exception:
    fuzz = None

def en_misspellings(text: str):
    if SP_EN is None:
        return []
    out = []
    for m in re.finditer(r"[A-Za-z][A-Za-z\-']+", text):
        w = m.group(0)
        if w.lower() in SP_EN.unknown([w]):
            cand = list(SP_EN.candidates(w))
            out.append({"start": m.start(), "end": m.end(), "word": w, "suggest": cand[:3]})
    return out

def fi_misspellings_voikko(text: str):
    if _VOIKKO is None:
        return []
    out = []
    for m in re.finditer(r"[A-Za-z√Ö√Ñ√ñ√•√§√∂][A-Za-z√Ö√Ñ√ñ√•√§√∂\-']+", text):
        w = m.group(0)
        if not _VOIKKO.spell(w):
            sugg = _VOIKKO.suggest(w) or []
            out.append({"start": m.start(), "end": m.end(), "word": w, "suggest": sugg[:3]})
    return out

NUMERIC_RE = re.compile(
    r"""
    [-+]?
    (?:
        \d{1,3}(?:\.\d{3})+(?:,\d+)? |  # thousand separators with optional decimal comma
        \d+(?:[.,]\d+)?                   # plain number with optional decimal part
    )
    (?:\s*[\u2013-]\s*[-+]?(?:\d{1,3}(?:\.\d{3})+(?:,\d+)?|\d+(?:[.,]\d+)?))?  # optional range
    %?                                      # optional percentage
    """,
    re.VERBOSE,
)


def _extract_numbers(text: str) -> List[str]:
    """Return all numeric-like substrings from text."""
    return NUMERIC_RE.findall(text)

def slm_cleanup(text: str, translate_embedded: bool, **kwargs) -> Dict:
    """Adapter around the low level ``_slm_cleanup`` function.

    The wrapper forwards any additional keyword arguments to the underlying
    implementation and ensures that a JSON object with the expected schema is
    always returned.  If the model fails to produce valid JSON, the input text
    is split into sentence-like parts and processed piece by piece.
    """

    llama = kwargs.get("llama", _load_llama())
    gen = {
        "llama": llama,
        "temp": kwargs.get("temp", TEMP),
        "max_tokens": kwargs.get("max_tokens", MAX_TOKENS),
    }

    def _call(t: str) -> Dict:
        try:
            raw = _slm_cleanup(t, translate_embedded, **gen)
        except TypeError:
            raw = _slm_cleanup(t, translate_embedded)

        if isinstance(raw, dict):
            raw = json.dumps(raw)
        return extract_json(raw)

    try:
        return _call(text)
    except Exception:
        parts = [m.group(0) for m in re.finditer(r"[^.!?]+[.!?]?\s*", text)]
        clean_parts: List[str] = []
        flags: List = []
        changes: List = []
        offset = 0
        for part in parts:
            try:
                res = _call(part)
            except Exception:
                # If even individual parts cannot be parsed, fall back to
                # returning the original input verbatim.
                return {"clean_text": text, "flags": [], "changes": []}
            ct = res.get("clean_text", "")
            clean_parts.append(ct)
            for f in res.get("flags", []):
                item = f
                if isinstance(item, dict) and "span" in item:
                    s, e = item["span"]
                    item = {**item, "span": [s + offset, e + offset]}
                flags.append(item)
            for c in res.get("changes", []):
                item = c
                if isinstance(item, dict) and "span" in item:
                    s, e = item["span"]
                    item = {**item, "span": [s + offset, e + offset]}
                changes.append(item)
            offset += len(ct)
        return {"clean_text": "".join(clean_parts), "flags": flags, "changes": changes}


def normalize_flags_and_changes(result: Dict, masked: str) -> Dict:
    flags_raw = result.get("flags", [])
    if not isinstance(flags_raw, list):
        flags_raw = []
    normalized_flags: List[Dict] = []
    numeric_change = False
    for f in flags_raw:
        if isinstance(f, dict):
            if f.get("type") == "numeric_change":
                numeric_change = True
            normalized_flags.append(f)
        elif isinstance(f, str):
            if f == "numeric_change":
                numeric_change = True
            normalized_flags.append({"type": f})
        # silently drop other types

    if numeric_change:
        result["clean_text"] = masked
        normalized_flags = [f for f in normalized_flags if f.get("type") != "numeric_change"]
        normalized_flags.append({"type": "numeric_change"})
        result["changes"] = []

    result["flags"] = normalized_flags

    changes_raw = result.get("changes", [])
    if not isinstance(changes_raw, list):
        changes_raw = []
    result["changes"] = [c for c in changes_raw if isinstance(c, dict)]

    return result

def _similarity(a: str, b: str) -> float:
    if fuzz:
        return fuzz.ratio(a, b) / 100.0
    return 1.0


def run_pipeline(
    text: str,
    translate_embedded: bool = False,
    protected_terms: Optional[List[str]] = None,
    record_id: Optional[str] = None,
    correlation_id: Optional[str] = None,
) -> Dict:
    log, cid = get_logger(correlation_id or record_id)
    log = log.bind(record_id=record_id or cid)
    log.info("pipeline_start", event="pipeline_start", input_length=len(text))

    locks = extract_entities(text)
    masked = mask_terms(text, protected_terms or [])

    spans = lang_spans(masked)
    langs = {s['lang'] for s in spans}
    flags: List[Dict] = []
    mixed_languages = len(langs) > 1
    if 'en' in langs and 'fi' in langs:
        flags.append({'type': 'embedded_en'})

    spell_changes: List[Dict] = []
    for s in spans:
        if s["lang"].startswith("en"):
            for m in en_misspellings(s["text"]):
                spell_changes.append({
                    "span": [s["start"] + m["start"], s["start"] + m["end"]],
                    "type": "spelling",
                    "source": "spell",
                    "before": m["word"],
                    "after": (m["suggest"][0] if m["suggest"] else m["word"])
                })
        if s["lang"].startswith("fi"):
            for m in fi_misspellings_voikko(s["text"]):
                spell_changes.append({
                    "span": [s["start"] + m["start"], s["start"] + m["end"]],
                    "type": "spelling",
                    "source": "voikko",
                    "before": m["word"],
                    "after": (m["suggest"][0] if m["suggest"] else m["word"])
                })

    llama = _load_llama()
    # The stubbed slm_cleanup ignores the llama and generation parameters,
    # but the real implementation will use them.

    try:
        result = slm_cleanup(
            masked,
            translate_embedded,
            llama=llama,
            temp=TEMP,
            max_tokens=MAX_TOKENS,
        )
    except TypeError:
        # Allow monkeypatched or legacy implementations that don't accept kwargs
        result = slm_cleanup(masked, translate_embedded)
    validate_json_schema(result)

    try:
        forbid_changes_in_terms(masked, result['clean_text'])
    except ValueError:
        # LLM touched locked content; revert and flag below
        result['clean_text'] = masked

    enforced_clean, entity_flags = enforce_entity_lock(masked, result['clean_text'], locks)
    if enforced_clean != result['clean_text']:
        log.warning("entity_lock_enforced", event="entity_lock_enforced", record_id=record_id or cid)
    result['clean_text'] = enforced_clean

    flags.extend(result.get('flags', []))
    flags.extend(entity_flags)
    # Normalise flags to dictionaries and ignore any numeric change markers
    # coming from the model itself; numeric diffs are detected separately.
    normalised: List[Dict] = []
    for f in flags:
        if isinstance(f, str):
            if f == 'numeric_change':
                continue
            normalised.append({'type': f})
        elif isinstance(f, dict):
            if f.get('type') == 'numeric_change':
                continue
            normalised.append(f)
    flags = normalised
    if _extract_numbers(masked) != _extract_numbers(result.get('clean_text', '')):
        flags.append({'type': 'numeric_change'})

    changes = result.get('changes', [])
    changes.extend(spell_changes)
    if result['clean_text'] != masked:
        matcher = difflib.SequenceMatcher(a=masked, b=result['clean_text'])
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                continue
            changes.append({
                'source': 'diff',
                'type': 'rewrite',
                'span': [i1, i2],
                'before': masked[i1:i2],
                'after': result['clean_text'][j1:j2],
            })

    risk_score = _similarity(masked, result['clean_text'])
    if risk_score < 0.85:
        flags.append({'type': 'high_risk_rewrite', 'score': round(risk_score, 3)})

    review_status = "auto_approved"
    if any(f.get('type') in {'high_risk_rewrite', 'numeric_change', 'locked_entity_changed'} for f in flags if isinstance(f, dict)):
        review_status = "pending"

    final = {
        'clean_text': result['clean_text'],
        'flags': flags,
        'changes': changes,
        'mixed_languages': mixed_languages,
        'risk_score': float(risk_score),
        'review_status': review_status,
    }
    out = normalize_flags_and_changes(final, masked)
    log.info("pipeline_end", event="pipeline_end", record_id=record_id or cid, risk_score=risk_score, review_status=review_status)
    return out

def run_pipeline_like_this():
    example = "T√§m√§ takki on super warm for winter commutes kaupungilla."
    return run_pipeline(example)


--------------------------------------------------------------------------------

================================================================================
FILE: app\review_queue.py
================================================================================

import json
from pathlib import Path
from typing import Any, Dict, Optional

REVIEW_FILE = Path("data/review_queue.jsonl")


def _ensure_file() -> None:
    REVIEW_FILE.parent.mkdir(parents=True, exist_ok=True)
    REVIEW_FILE.touch(exist_ok=True)


def enqueue(item_id: str, payload: Dict[str, Any]) -> None:
    _ensure_file()
    with REVIEW_FILE.open("a", encoding="utf-8") as f:
        record = {"id": item_id, "status": "pending", **payload}
        f.write(json.dumps(record, ensure_ascii=False) + "\n")


def update(item_id: str, approved: bool, correction: Optional[str] = None) -> Dict[str, Any]:
    _ensure_file()
    updated = []
    found = None
    with REVIEW_FILE.open("r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                continue
            if obj.get("id") == item_id and found is None:
                obj["status"] = "approved" if approved else "rejected"
                if correction:
                    obj["correction"] = correction
                found = obj
            updated.append(obj)

    if found is None:
        # Create a new entry if it wasn't in the queue yet
        found = {"id": item_id, "status": "approved" if approved else "rejected"}
        if correction:
            found["correction"] = correction
        updated.append(found)

    with REVIEW_FILE.open("w", encoding="utf-8") as f:
        for obj in updated:
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
    return found


--------------------------------------------------------------------------------

================================================================================
FILE: app\schemas.py
================================================================================

from typing import Any, List, Optional
from pydantic import BaseModel


class CleanRequest(BaseModel):
    id: Optional[str] = None
    text: str
    terms: Optional[List[str]] = None
    translate_embedded: bool = False


class Change(BaseModel):
    source: str
    type: str
    original: str
    corrected: str


class CleanResponse(BaseModel):
    clean_text: str
    flags: List[Any]
    changes: List[Any]
    risk_score: float
    review_status: str


class ReviewRequest(BaseModel):
    approved: bool
    correction: Optional[str] = None


--------------------------------------------------------------------------------

================================================================================
FILE: app\server.py
================================================================================

from fastapi import FastAPI
from .schemas import CleanRequest, CleanResponse, ReviewRequest
from .pipeline import run_pipeline
from .review_queue import update as update_review, enqueue as enqueue_review

app = FastAPI()
MODEL_READY = True


@app.get('/healthz')
def healthz():
    return {'status': 'ok', 'model_loaded': MODEL_READY}


@app.post('/clean', response_model=CleanResponse)
def clean(req: CleanRequest):
    result = run_pipeline(
        req.text,
        translate_embedded=req.translate_embedded,
        protected_terms=req.terms,
        record_id=req.id,
    )
    if result.get("review_status") == "pending":
        enqueue_review(str(req.id or ""), {"text": req.text, "flags": result.get("flags"), "changes": result.get("changes")})
    return CleanResponse(**result)


@app.post('/review/{item_id}')
def review(item_id: str, body: ReviewRequest):
    """Human-in-the-loop review endpoint."""
    updated = update_review(item_id, approved=body.approved, correction=body.correction)
    return updated


--------------------------------------------------------------------------------

================================================================================
FILE: app\slm_llamacpp.py
================================================================================

"""Wrapper utilities for llama-cpp based small language model."""

from __future__ import annotations

import json
import re
from typing import Any, Dict

from .guardrails import JSON_END, JSON_START, extract_json

try:  # optional dependency
    from llama_cpp import Llama  # type: ignore
except Exception:  # pragma: no cover - llama_cpp is optional
    Llama = None  # type: ignore

# Generation system prompt and JSON sentinels
SYSTEM = (
    "Olet kielipuhdistusagentti. A,lA muuta merkitystA. "
    "A,lA muuta <TERM>∆í?ƒ∞</TERM>-sisAltAA. Vastaa AINOASTAAN JSONILLA."
)


def _build_user(masked_text: str, translate_embedded: bool) -> str:
    """Return user prompt for the model."""
    return (
        f"""Kontekstikieli: FI. Sallitut kielet: FI ja EN.
Ohjeet:
- Korjaa kielioppi ja vAlimerkit.
- Jos FI-tekstissA on upotettu EN-segmentti, lisAA flags: {{ "type":"embedded_en","start":i,"end":j }}.
- translate_embedded = {"true" if translate_embedded else "false"} ∆í+' jos true, kAAnnA EN-osiot suomeksi.
- A,lA muuta <TERM>∆í?ƒ∞</TERM> -osuuksia.
- Palauta VAIN JSON seuraavan skeeman mukaan, ilman mitAAn muuta tekstiA:
{JSON_START}{{"clean_text":"...","flags":[{{"type":"embedded_en","start":0,"end":0}}],"changes":[{{"span":[0,0],"type":"grammar|spelling|punctuation|translation","source":"slm|spell|voikko","before":"","after":""}}]}}{JSON_END}

TEKSTI:
{masked_text}
"""
    )


def slm_cleanup(masked_text: str, translate_embedded: bool, **kwargs: Any) -> Dict:
    """Clean up text using an optional ``llama`` instance.

    Parameters are accepted as ``**kwargs`` so that unused generation
    parameters (e.g. ``temperature`` or ``max_tokens``) do not raise errors.
    When ``llama`` is ``None`` the function acts as a deterministic stub
    returning the original ``masked_text``.
    """

    llama = kwargs.get("llama")
    temperature = kwargs.get("temperature", kwargs.get("temp", 0.0))
    max_tokens = kwargs.get("max_tokens", 512)

    def _call(t: str) -> Dict:
        """Generate and parse model output for ``t``."""

        if llama is None or not hasattr(llama, "create_chat_completion"):
            # Deterministic stub used in tests
            raw = (
                JSON_START
                + json.dumps({"clean_text": t, "flags": [], "changes": []}, ensure_ascii=False)
                + JSON_END
            )
        else:  # pragma: no cover - requires llama_cpp
            try:
                prompt = _build_user(t, translate_embedded)
                out = llama.create_chat_completion(
                    messages=[
                        {"role": "system", "content": SYSTEM},
                        {"role": "user", "content": prompt},
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                raw = out["choices"][0]["message"]["content"]
            except Exception:
                raw = (
                    JSON_START
                    + json.dumps({"clean_text": t, "flags": [], "changes": []}, ensure_ascii=False)
                    + JSON_END
                )
        return extract_json(raw)

    try:
        return _call(masked_text)
    except Exception:
        # Fallback: process sentence by sentence
        parts = [m.group(0) for m in re.finditer(r"[^.!?]+[.!?]?\s*", masked_text)]
        clean_parts = []
        flags = []
        changes = []
        offset = 0
        for part in parts:
            res = _call(part)
            ct = res.get("clean_text", "")
            clean_parts.append(ct)

            for f in res.get("flags", []):
                if isinstance(f, dict):
                    if {"start", "end"}.issubset(f.keys()):
                        flags.append({**f, "start": f["start"] + offset, "end": f["end"] + offset})
                    elif "span" in f:
                        s, e = f["span"]
                        flags.append({**f, "span": [s + offset, e + offset]})
                    else:
                        flags.append(f)
                else:
                    flags.append(f)

            for c in res.get("changes", []):
                if isinstance(c, dict) and "span" in c:
                    s, e = c["span"]
                    changes.append({**c, "span": [s + offset, e + offset]})
                else:
                    changes.append(c)

            offset += len(ct)

        return {"clean_text": "".join(clean_parts), "flags": flags, "changes": changes}


--------------------------------------------------------------------------------

================================================================================
FILE: app\spellcheck.py
================================================================================

from typing import List, Dict


def load_hunspell(lang_code: str):
    """Stub for loading hunspell dictionaries."""
    return None


def misspellings(text: str, lang_code: str) -> List[Dict]:
    """Return list of misspellings; stub returns empty list."""
    return []


--------------------------------------------------------------------------------

================================================================================
FILE: app\__init__.py
================================================================================



--------------------------------------------------------------------------------

================================================================================
FILE: cli\clean_file.py
================================================================================

import json
from pathlib import Path
import click

from app.pipeline import run_pipeline


@click.command()
@click.argument('input_path', type=click.Path(exists=True))
@click.option('-o', '--output', type=click.Path(), help='Output JSON file')
def main(input_path, output):
    text = Path(input_path).read_text(encoding='utf-8')
    result = run_pipeline(text)
    out_json = json.dumps(result, ensure_ascii=False, indent=2)
    if output:
        Path(output).write_text(out_json, encoding='utf-8')
    clean_path = Path(input_path).with_name(Path(input_path).stem + '-clean.txt')
    clean_path.write_text(result['clean_text'], encoding='utf-8')
    click.echo(out_json)


if __name__ == '__main__':
    main()


--------------------------------------------------------------------------------

================================================================================
FILE: cli\clean_folder.py
================================================================================

import json
from pathlib import Path
import click

from app.pipeline import run_pipeline


@click.command()
@click.argument('folder', type=click.Path(exists=True))
def main(folder):
    folder_path = Path(folder)
    for txt_file in folder_path.glob('*.txt'):
        text = txt_file.read_text(encoding='utf-8')
        result = run_pipeline(text)
        clean_path = txt_file.with_name(txt_file.stem + '-clean.txt')
        flag_path = txt_file.with_name(txt_file.stem + '-flags.json')
        clean_path.write_text(result['clean_text'], encoding='utf-8')
        flag_path.write_text(json.dumps(result, ensure_ascii=False, indent=2), encoding='utf-8')


if __name__ == '__main__':
    main()


--------------------------------------------------------------------------------

================================================================================
FILE: cli\clean_table.py
================================================================================

import argparse
import os
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from app.io_utils import read_table, write_table, parse_terms, serialize
from app.checkpointing import Checkpointer
from app.review_queue import enqueue as enqueue_review
from app.logging_utils import get_logger


def main() -> None:
    ap = argparse.ArgumentParser(description="Batch-clean CSV/Excel table")
    ap.add_argument(
        "input",
        help="Input CSV/Excel with columns: id,text,(optional)protected_terms,(optional)translate_embedded",
    )
    ap.add_argument(
        "-o",
        "--output",
        help="Output path (.csv or .xlsx). Default: <input>.clean.csv",
        default=None,
    )
    ap.add_argument(
        "--model-path",
        default=None,
        help="Path to .gguf model (overrides $MODEL_PATH)",
    )
    ap.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of worker threads (default 4)",
    )
    args = ap.parse_args()

    mp = args.model_path or os.environ.get("MODEL_PATH")
    if not mp or not Path(mp).exists():
        raise SystemExit(
            "MODEL_PATH is not set or file not found. Use --model-path or export MODEL_PATH=<path/to/model.gguf>"
        )
    os.environ["MODEL_PATH"] = str(mp)
    t0 = time.time()

    from app.pipeline import run_pipeline

    inp = Path(args.input)
    out = Path(args.output) if args.output else inp.with_suffix(".clean.csv")

    df = read_table(str(inp))
    if "text" not in df.columns:
        raise SystemExit("Input must contain column 'text'")

    has_terms = "protected_terms" in df.columns
    has_translate = "translate_embedded" in df.columns
    has_id = "id" in df.columns

    base_columns = list(df.columns)
    extra_columns = ["clean_text", "flags", "changes", "mixed_languages", "risk_score", "review_status"]
    all_columns = base_columns + [c for c in extra_columns if c not in base_columns]

    use_checkpoint = out.suffix.lower() == ".csv" and has_id
    checkpointer = None
    if use_checkpoint:
        checkpointer = Checkpointer(out, out.with_suffix(".errors.csv"), id_field="id", fieldnames=all_columns)

    clean_texts: list[str] = []
    flags_col: list[str] = []
    changes_col: list[str] = []
    mixed_col: list[bool] = []
    risk_scores: list[float] = []
    review_statuses: list[str] = []
    flag_stats: dict[str, int] = {"embedded_en": 0, "term_change": 0}

    def process_row(row: dict) -> dict:
        text = str(row["text"])
        terms = parse_terms(row.get("protected_terms")) if has_terms else []
        translate = bool(row.get("translate_embedded")) if has_translate else False
        row_id = row.get("id")
        res = run_pipeline(
            text,
            translate_embedded=translate,
            protected_terms=terms,
            record_id=str(row_id) if row_id is not None else None,
        )
        if res.get("review_status") == "pending":
            enqueue_review(str(row_id or ""), {"text": text, "flags": res.get("flags"), "changes": res.get("changes")})
        return res

    rows = df.to_dict("records")
    chunk_size = max(1, args.workers * 4)
    processed_count = 0
    skipped = 0
    log, _ = get_logger()
    with ThreadPoolExecutor(max_workers=args.workers) as ex:
        for i in range(0, len(rows), chunk_size):
            chunk = rows[i : i + chunk_size]
            to_process = []
            for row in chunk:
                if use_checkpoint and checkpointer and checkpointer.is_processed(row.get("id")):
                    skipped += 1
                    continue
                to_process.append(row)
            results = ex.map(process_row, to_process)
            for row, res in zip(to_process, results):
                out_row = {**row}
                out_row["clean_text"] = res["clean_text"]
                out_row["flags"] = serialize(res["flags"])
                out_row["changes"] = serialize(res["changes"])
                out_row["mixed_languages"] = res["mixed_languages"]
                out_row["risk_score"] = res.get("risk_score", 1.0)
                out_row["review_status"] = res.get("review_status", "auto_approved")

                if use_checkpoint and checkpointer:
                    try:
                        checkpointer.append_row({k: out_row.get(k) for k in all_columns})
                    except Exception as exc:
                        checkpointer.append_error(row.get("id"), str(exc), row.get("text", ""))
                else:
                    clean_texts.append(out_row["clean_text"])
                    flags_col.append(serialize(out_row["flags"]))
                    changes_col.append(serialize(out_row["changes"]))
                    mixed_col.append(out_row["mixed_languages"])
                    risk_scores.append(out_row["risk_score"])
                    review_statuses.append(out_row["review_status"])

                for f in res["flags"]:
                    t = f.get("type") if isinstance(f, dict) else f
                    if t:
                        flag_stats[t] = flag_stats.get(t, 0) + 1
                processed_count += 1
            if (i + len(chunk)) % 500 == 0:
                log.info("batch_progress", event="batch_progress", processed=processed_count, skipped=skipped)

    if not use_checkpoint:
        df["clean_text"] = clean_texts
        df["flags"] = flags_col
        df["changes"] = changes_col
        df["mixed_languages"] = mixed_col
        df["risk_score"] = risk_scores
        df["review_status"] = review_statuses
        write_table(df, str(out))

    total = processed_count
    flag_count = sum(flag_stats.values())
    elapsed = time.time() - t0
    elapsed_ms = int(elapsed * 1000)
    throughput = total / elapsed if elapsed > 0 else 0
    summary = ", ".join(f"{k}={v}" for k, v in sorted(flag_stats.items()))
    log.info(
        "batch_complete",
        event="batch_complete",
        processed=total,
        skipped=skipped,
        flags=flag_count,
        elapsed_ms=elapsed_ms,
        throughput_rps=throughput,
        output=str(out),
        summary=summary,
    )


if __name__ == "__main__":
    main()


--------------------------------------------------------------------------------

================================================================================
FILE: docs\design_document.md
================================================================================

# SLM Cleanroom ‚Äî Design Document

## 1. Purpose & Scope
**Goal:** Local pipeline that cleans mixed-language texts (FI focus, EN embedded), fixes spelling/grammar, protects `<TERM>‚Ä¶</TERM>` and numbers, and returns an auditable diff (`flags`, `changes`) at batch scale (CSV/Excel).
**In scope**
- Batch processing for CSV/Excel
- Single string API (`/clean`) + healthcheck
- Local model (GGUF via llama.cpp) w/ ‚Äúdownload if missing‚Äù
- Basic EN spellcheck (pyspellchecker), optional FI via Voikko
- Guardrails for terms & numerics; valid JSON output, auto-retry on JSON fail
**Out of scope (for now)**
- Rich translation / summarization / reasoning
- Heavy UI (only light Streamlit reviewer)
- Non-FI/EN languages beyond basic detection
- Multi-agent autonomy

## 2. Functional Requirements
- `run_pipeline(text, translate_embedded, protected_terms)` ‚Üí returns:
  ```json
  {
    "clean_text": "string",
    "flags": [{"type": "...", "start": 0, "end": 0, "text": "opt"}],
    "changes": [{"span": [0,0], "type":"spelling|grammar|punctuation|translation", "source":"spell|slm|voikko", "before":"", "after":""}],
    "lang_spans": [{"start":0,"end":0,"lang":"fi|en","text":"..."}],
    "mixed_languages": true
  }


CLI: python cli/clean_table.py <in.csv/xlsx> -o <out> [--model-path PATH] [--workers N]

API: FastAPI GET /healthz, POST /clean

Colab demo: end-to-end single + batch

Model download: HF repo/file, only-if-missing

3. Non-Functional Requirements

Reproducibility: Dockerfile + devcontainer (Phase 3)

Performance targets (baseline, CPU/Colab):

JSON validity ‚â• 99.9%

Batch 500+ rows / 10 min with TinyLlama

Quality targets:

TERM & numeric invariance = 100%

Embedded-EN detection precision ‚â• 95% on mock/golden

Extensibility: swap model via MODEL_PATH; knobs in app/config.py

4. Architecture & Directories
app/
  pipeline.py        # detection ‚Üí spell/voikko ‚Üí SLM cleanup ‚Üí guardrails
  guardrails.py      # JSON extraction, schema checks
  model_download.py  # HF download-if-missing
  io_utils.py        # CSV/Excel I/O
  config.py          # runtime knobs (CTX, TEMP, etc.)
  server.py          # FastAPI
cli/
  clean_table.py
  clean_file.py      # (optional later)
data/
  mock_inputs.csv
  golden_inputs.csv
  golden_expected.jsonl
notebooks/
  colab_demo.ipynb
tests/
  test_pipeline_smoke.py
  test_batch_smoke.py
  test_golden.py
tools/
  bench.py
ui/
  app.py             # (optional Streamlit)
Dockerfile
requirements.txt
README.md

5. Models

Default: TinyLlama-1.1B-Instruct (fast)

Better: Mistral-7B-Instruct-v0.3 Q4_K_M

Alt: Llama-3.1-8B-Instruct Q4_K_M
Switch by setting MODEL_PATH. Download helpers live in app/model_download.py.

6. Guardrails & Validation

<TERM>‚Ä¶</TERM> content must remain byte-identical.

Numbers (including signed, %, ranges ‚Äú12‚Äì15‚Äù, decimals ‚Äú3,5‚Äù) must not change.

JSON strict: clean_text (str), flags (list), changes (list).

Auto-retry on JSON parse failure: smaller chunk (sentence level), then stitch, offset-correct flags/changes.

7. Spellchecking Strategy

EN: pyspellchecker fallback (token-level suggestions).

FI: optional Voikko (python3-libvoikko, voikko-fi) when available.

Merge into changes with source: "spell" | "voikko".

8. Batch Processing

CSV/Excel via pandas/openpyxl.

Optional parallelism (--workers, default 4). Preserve input order.

Output columns added: clean_text, flags, changes, mixed_languages.

9. QA & Regression

Mock set: 10 varied rows (FI/EN mix, typos, TERM, numbers).

Golden set: inputs + expected flags_hash + invariants.

Pytest must pass without model download (mock SLM in CI).

Bench script tools/bench.py for latency/throughput & retry rate.

10. Roadmap

Phase 1 (MVP): pipeline, CLI, API, Colab, mock
Phase 2: JSON retry + guardrails, parallel batch, config/logging, golden tests
Phase 3: Docker/devcontainer, Streamlit reviewer
Phase 4: FI Voikko enabled by default where available, benchmarking & tuning

11. Change Control

Every PR must reference this document: list section(s) changed and rationale.

If a change affects scope/requirements/architecture, update this doc in the same PR.


**Acceptance**
- File `docs/design_document.md` created exactly as above.
- No other files changed in this PR.


--------------------------------------------------------------------------------

================================================================================
FILE: docs\runbook.md
================================================================================

# SLM Cleanroom ‚Äî Run Book

> This run book shows how to set up and run the project locally, in GitHub Codespaces, in Docker, and in Google Colab.  
> **Design Document:** see [docs/design_document.md](design_document.md). All changes must reference it.

---

## 0) Model choices (GGUF)
Free, small to large (pick one):
- **TinyLlama-1.1B-1T-instruct.Q4_K_M.gguf** (fast baseline)
- **Mistral-7B-Instruct-v0.3.Q4_K_M.gguf** (better quality)
- **Llama-3.1-8B-Instruct.Q4_K_M.gguf** (higher quality)

---

## 1) Local / Codespace

### 1.1. Environment
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

1.2. Download model (only if missing)
# pick a model
export HF_REPO_ID="bartowski/TinyLlama-1.1B-1T-GGUF"
export HF_FILENAME="TinyLlama-1.1B-1T-instruct.Q4_K_M.gguf"

python -m app.model_download     # downloads models/$HF_FILENAME if missing
export MODEL_PATH="$PWD/models/$HF_FILENAME"

1.3. Smoke tests
PYTHONPATH=. pytest -q
python -m cli.clean_table data/mock_inputs.csv -o data/mock_outputs.csv --model-path "$MODEL_PATH"

1.4. API
uvicorn app.server:app --reload --port 8000
# in another shell:
curl -s -X POST http://localhost:8000/clean -H "content-type: application/json" \
 -d '{"text":"Takki ‚Äì super warm for winter commutes!","translate_embedded":true}' | jq .

```

2) Docker (reproducible)
2.1. Build image
docker build -t slm-cleanroom:latest .

2.2. Run batch (bind-mount model + repo)
# Make sure a model file exists in ./models first
docker run --rm \
  -v "$(pwd)":/app \
  -v "$(pwd)/models":/models \
  -e MODEL_PATH=/models/TinyLlama-1.1B-1T-instruct.Q4_K_M.gguf \
  -w /app slm-cleanroom:latest \
  bash -lc 'python cli/clean_table.py data/mock_inputs.csv -o /app/data/mock_outputs.csv'

3) Google Colab (with Drive model auto-download)
3.1. Install dependencies
!pip -q install llama-cpp-python langid rapidfuzz python-levenshtein \
               pyspellchecker huggingface_hub pandas openpyxl

3.2. Mount Drive
from google.colab import drive
drive.mount('/content/drive')

3.3. Download model to Drive only if missing
from pathlib import Path
import os, shutil
from huggingface_hub import hf_hub_download

# Choose model
REPO_ID = "bartowski/TinyLlama-1.1B-1T-GGUF"
FILENAME = "TinyLlama-1.1B-1T-instruct.Q4_K_M.gguf"

DRIVE_ROOT = Path("/content/drive/MyDrive")
MODELS_DIR = DRIVE_ROOT / "slm_cleanroom" / "models"
MODELS_DIR.mkdir(parents=True, exist_ok=True)

MODEL_PATH = MODELS_DIR / FILENAME

if MODEL_PATH.exists():
    print("‚úÖ Model already in Drive:", MODEL_PATH)
else:
    print("‚¨áÔ∏è Downloading model to Colab tmp‚Ä¶")
    tmp = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, local_dir="/content")
    shutil.copy2(tmp, MODEL_PATH)
    print("‚úÖ Saved to Drive:", MODEL_PATH)

os.environ["MODEL_PATH"] = str(MODEL_PATH)
MODEL_PATH

3.4. Import project code

Upload/clone the repository into /content/slm-cleanroom (or use Colab‚Äôs file browser). Then:

import sys, os
sys.path.append("/content/slm-cleanroom")  # adjust if different
from app.pipeline import run_pipeline

3.5. Single-string smoke test
res = run_pipeline(
    "T√§m√§ takki on NorthFace 1996 retro down jacket ‚Äì super warm for winter commutes!",
    translate_embedded=True,
    protected_terms=["NorthFace 1996"]
)
res["clean_text"], res["flags"][:3]

3.6. Batch on CSV and save to Drive
from cli.clean_table import main as batch_main
import sys, pandas as pd, os

# Ensure MODEL_PATH is inherited
print("MODEL_PATH =", os.environ.get("MODEL_PATH"))

# Run batch on mock CSV inside repo
sys.argv = ["clean_table.py", "/content/slm-cleanroom/data/mock_inputs.csv",
            "-o", "/content/mock_outputs.csv", "--model-path", os.environ["MODEL_PATH"]]
batch_main()

# Save outputs to Drive
import shutil
OUT_DRIVE = "/content/drive/MyDrive/slm_cleanroom/outputs/mock_outputs.csv"
shutil.copy("/content/mock_outputs.csv", OUT_DRIVE)
print("‚úÖ Saved:", OUT_DRIVE)

pd.read_csv("/content/mock_outputs.csv").head()

4) Configuration & Tuning

You can control runtime via env vars (see app/config.py):

MODEL_PATH ‚Äî path to .gguf

N_THREADS (default 8), CTX (default 8192)

TEMP (default 0.0), MAX_TOKENS (default 512)

Example:

export N_THREADS=12 CTX=4096 TEMP=0.0 MAX_TOKENS=512

5) Quality & Guardrails

TERM invariance: <TERM>‚Ä¶</TERM> content must be identical pre/post.

Numerics: signed numbers, percentages, ranges (e.g., 12‚Äì15), and decimals (3,5) must not change.

JSON validity: pipeline retries with smaller chunks if JSON parsing fails.

Flags: at least embedded_en, term_change, numeric_change are tracked.

Run tests:

PYTHONPATH=. pytest -q

6) Troubleshooting

MODEL_PATH is not set or file not found
‚Üí Export MODEL_PATH or pass --model-path to CLI.

ValueError: HF_FILENAME must be a model artifact
‚Üí Use a .gguf/.ggml/.bin file, not README.md.

Colab OOM / very slow
‚Üí Use TinyLlama instead of 7B/8B models; reduce CTX.

JSON parse errors
‚Üí The pipeline‚Äôs retry logic should handle it. If persistent, reduce chunk size or try a different model.

7) Operational Tips

For large files, use --workers (e.g., 4‚Äì8) to increase throughput:

python cli/clean_table.py big.csv -o big.clean.csv \
  --model-path "$MODEL_PATH" --workers 6


Use tools/bench.py for quick perf sampling (if present):

python tools/bench.py --file data/mock_inputs.csv --samples 200 --workers 4


Streamlit review (if present):

streamlit run ui/app.py


**(Optional) Create** `notebooks/colab_bootstrap.py` **to mirror the Drive model ‚Äúdownload-if-missing‚Äù logic (importable in notebooks):**
```python
# notebooks/colab_bootstrap.py
from pathlib import Path
import os, shutil
from typing import Optional
from huggingface_hub import hf_hub_download

def ensure_drive_model(repo_id: str, filename: str, drive_subdir: str = "slm_cleanroom/models") -> str:
    drive_root = Path("/content/drive/MyDrive")
    models_dir = drive_root / drive_subdir
    models_dir.mkdir(parents=True, exist_ok=True)
    dest = models_dir / filename
    if dest.exists():
        return str(dest)
    tmp = hf_hub_download(repo_id=repo_id, filename=filename, local_dir="/content")
    shutil.copy2(tmp, dest)
    return str(dest)

def set_model_env(path: str):
    os.environ["MODEL_PATH"] = path
    return path
```


--------------------------------------------------------------------------------

================================================================================
FILE: notebooks\colab_bootstrap.py
================================================================================

from pathlib import Path
import os, shutil
from typing import Optional
from huggingface_hub import hf_hub_download

def ensure_drive_model(repo_id: str, filename: str, drive_subdir: str = "slm_cleanroom/models") -> str:
    drive_root = Path("/content/drive/MyDrive")
    models_dir = drive_root / drive_subdir
    models_dir.mkdir(parents=True, exist_ok=True)
    dest = models_dir / filename
    if dest.exists():
        return str(dest)
    tmp = hf_hub_download(repo_id=repo_id, filename=filename, local_dir="/content")
    shutil.copy2(tmp, dest)
    return str(dest)

def set_model_env(path: str):
    os.environ["MODEL_PATH"] = path
    return path


--------------------------------------------------------------------------------

================================================================================
FILE: tests\test_batch_smoke.py
================================================================================

import os
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.pipeline import run_pipeline

def test_embedded_en_flag():
    txt = "Takki on l√§mmin ‚Äì super warm for winter commutes!"
    res = run_pipeline(txt, translate_embedded=False)
    assert any(f.get("type")=="embedded_en" for f in res["flags"]) or res["mixed_languages"]

def test_term_protection():
    txt = "Malli <TERM>ABC-123</TERM> sopii t√§h√§n."
    res = run_pipeline(txt, translate_embedded=True, protected_terms=["ABC-123"])
    assert "<TERM>ABC-123</TERM>" in res["clean_text"]


--------------------------------------------------------------------------------

================================================================================
FILE: tests\test_golden.py
================================================================================

import csv
import json
import hashlib
import os
import re
import string
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.pipeline import run_pipeline
from app.lang_utils import mask_terms

PUNCT = re.escape(string.punctuation)

def normalize(text: str) -> str:
    text = re.sub(rf'[{PUNCT}]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def flags_hash(flags) -> str:
    sorted_flags = sorted(flags, key=lambda f: json.dumps(f, sort_keys=True))
    flags_json = json.dumps(sorted_flags, sort_keys=True, separators=(',', ':'))
    return hashlib.sha256(flags_json.encode('utf-8')).hexdigest()

def test_golden_regression():
    data_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
    inp_path = os.path.join(data_dir, 'golden_inputs.csv')
    exp_path = os.path.join(data_dir, 'golden_expected.jsonl')

    with open(exp_path, encoding='utf-8') as f:
        expected = {int(obj['id']): obj for obj in map(json.loads, f)}

    with open(inp_path, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            rid = int(row['id'])
            text = row['text']
            terms = [t.strip() for t in row['protected_terms'].split('|')] if row['protected_terms'] else []
            translate = row['translate_embedded'].strip().lower() == 'true'
            result = run_pipeline(text, translate_embedded=translate, protected_terms=terms)

            # compare flags
            assert flags_hash(result['flags']) == expected[rid]['flags_hash']

            # normalized text comparison
            assert normalize(result['clean_text']) == normalize(expected[rid]['clean_text'])

            # invariants: numbers and protected terms
            masked_input = mask_terms(text, terms)
            for num in re.findall(r'-?\d+(?:[.,]\d+)?', masked_input):
                assert num in result['clean_text']
            for term in re.findall(r'<TERM>(.*?)</TERM>', masked_input):
                assert f'<TERM>{term}</TERM>' in result['clean_text']


--------------------------------------------------------------------------------

================================================================================
FILE: tests\test_invariants.py
================================================================================

import os
import sys
import pytest

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.pipeline import run_pipeline


def test_numeric_patterns_unchanged():
    text = "Values: -10%, range 12\u201315, decimal 3,5, big 1.000,50."
    result = run_pipeline(text)
    assert result["clean_text"] == text
    assert not any(f.get("type") == "numeric_change" for f in result["flags"])


def test_term_survives():
    text = "<TERM>ABC-123 v2</TERM> stays."
    result = run_pipeline(text)
    assert "<TERM>ABC-123 v2</TERM>" in result["clean_text"]


def test_numeric_change_flag(monkeypatch):
    def fake_cleanup(masked_text: str, translate_embedded: bool, **kwargs):
        return {"clean_text": masked_text.replace("-10%", "-11%"), "flags": [], "changes": []}

    monkeypatch.setattr("app.pipeline.slm_cleanup", fake_cleanup)
    result = run_pipeline("Discount -10% now")
    assert {"type": "numeric_change"} in result["flags"]


def test_term_change_raises(monkeypatch):
    def fake_cleanup(masked_text: str, translate_embedded: bool, **kwargs):
        return {"clean_text": masked_text.replace("ABC-123 v2", "XYZ-999"), "flags": [], "changes": []}

    monkeypatch.setattr("app.pipeline.slm_cleanup", fake_cleanup)
    with pytest.raises(ValueError):
        run_pipeline("<TERM>ABC-123 v2</TERM>")


--------------------------------------------------------------------------------

================================================================================
FILE: tests\test_json_and_invariance.py
================================================================================

import importlib

import app.slm_llamacpp as llm
import app.pipeline as pipeline
from app.pipeline import run_pipeline, slm_cleanup


def _patch_model(monkeypatch, func):
    """Helper to patch the low level model cleanup function."""
    monkeypatch.setattr(llm, "slm_cleanup", func)
    monkeypatch.setattr(pipeline, "_slm_cleanup", func)


def test_json_only_prompt_echo(monkeypatch):
    """Model echo without JSON should still yield valid structure."""

    def echo(masked_text: str, translate_embedded: bool, **_: object):
        return masked_text

    _patch_model(monkeypatch, echo)
    res = slm_cleanup("Just some text.", False)
    assert res == {"clean_text": "Just some text.", "flags": [], "changes": []}


def test_flags_shape_and_numeric_roll_back(monkeypatch):
    """String flags from the model are normalised and numeric_change dropped."""

    def mock(masked_text: str, translate_embedded: bool, **_: object):
        return {
            "clean_text": masked_text,
            "flags": ["embedded_en", "numeric_change"],
            "changes": [],
        }

    _patch_model(monkeypatch, mock)
    result = run_pipeline("Hello 10")
    assert result["flags"] == [{"type": "embedded_en"}]


def test_ctx_env(monkeypatch):
    """CTX defaults to 2048 but respects the environment variable."""

    import app.config as config

    monkeypatch.delenv("CTX", raising=False)
    importlib.reload(config)
    assert config.CTX == 2048

    monkeypatch.setenv("CTX", "123")
    importlib.reload(config)
    assert config.CTX == 123



--------------------------------------------------------------------------------

================================================================================
FILE: tests\test_pipeline_smoke.py
================================================================================

import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.pipeline import run_pipeline


def test_embedded_en_flag():
    text = "T√§m√§ takki on todella hyv√§ ja super warm for winter commutes kaupungilla."
    result = run_pipeline(text)
    assert any(f.get('type') == 'embedded_en' for f in result['flags'])


def test_term_unchanged():
    text = "Suosittu malli <TERM>NorthFace 1996</TERM> on klassikko."
    result = run_pipeline(text)
    assert '<TERM>NorthFace 1996</TERM>' in result['clean_text']


--------------------------------------------------------------------------------

================================================================================
FILE: tools\bench.py
================================================================================

import argparse
import time
import random
from collections import Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import os
import sys

import pandas as pd

ROOT = Path(__file__).resolve().parents[1]
sys.path.append(str(ROOT))

from app.pipeline import run_pipeline
from app.io_utils import parse_terms


MAX_RETRIES = 3


def _process_row(row):
    text = str(row.get("text", ""))
    terms = parse_terms(row.get("protected_terms")) if "protected_terms" in row else []
    translate = bool(row.get("translate_embedded", False))

    start = time.perf_counter()
    retries = 0
    while True:
        try:
            res = run_pipeline(text, translate_embedded=translate, protected_terms=terms)
            break
        except Exception:
            retries += 1
            if retries >= MAX_RETRIES:
                res = {"flags": [{"type": "error"}], "clean_text": text, "changes": []}
                break
    end = time.perf_counter()
    return (end - start), retries, res.get("flags", [])


def main():
    ap = argparse.ArgumentParser(description="Benchmark the cleaning pipeline")
    ap.add_argument("--file", required=True, help="Input CSV/Excel file with text column")
    ap.add_argument("--workers", type=int, default=1, help="Number of worker threads")
    ap.add_argument("--samples", type=int, default=200, help="Number of rows to sample")
    args = ap.parse_args()

    df = pd.read_csv(args.file) if Path(args.file).suffix.lower().endswith(".csv") else pd.read_excel(args.file)
    n = args.samples
    if n > len(df):
        sampled = df.sample(n=n, replace=True, random_state=random.randint(0, 1_000_000))
    else:
        sampled = df.sample(n=n, random_state=random.randint(0, 1_000_000))

    rows = sampled.to_dict("records")

    latencies = []
    total_retries = 0
    flag_counter = Counter()

    t0 = time.perf_counter()
    with ThreadPoolExecutor(max_workers=args.workers) as ex:
        futures = [ex.submit(_process_row, r) for r in rows]
        for fut in as_completed(futures):
            dur, retries, flags = fut.result()
            latencies.append(dur)
            total_retries += retries
            for f in flags:
                if isinstance(f, dict):
                    flag_counter[f.get("type", "?")] += 1
                else:
                    flag_counter[str(f)] += 1
    t1 = time.perf_counter()

    if not latencies:
        print("No rows processed")
        return

    lat_ms = [l * 1000 for l in latencies]
    lat_ms.sort()
    median = lat_ms[len(lat_ms)//2] if len(lat_ms)%2==1 else 0.5*(lat_ms[len(lat_ms)//2-1] + lat_ms[len(lat_ms)//2])
    p95_index = min(len(lat_ms)-1, int(len(lat_ms)*0.95))
    p95 = lat_ms[p95_index]

    total_time = t1 - t0
    throughput = len(lat_ms) / total_time if total_time > 0 else float('inf')
    retry_rate = total_retries / len(lat_ms)

    print(f"median latency: {median:.1f} ms")
    print(f"95p latency: {p95:.1f} ms")
    print(f"throughput: {throughput:.2f} rows/sec")
    print(f"JSON-retry rate: {retry_rate*100:.1f}%")
    if flag_counter:
        print("flag distribution:")
        for k, v in flag_counter.items():
            print(f"  {k}: {v}")
    else:
        print("flag distribution: none")


if __name__ == "__main__":
    main()


--------------------------------------------------------------------------------

================================================================================
FILE: ui\app.py
================================================================================

import streamlit as st
import pandas as pd

from app.pipeline import run_pipeline
from app.io_utils import parse_terms

st.title("SLM Cleanroom Review")

uploaded = st.file_uploader("Upload CSV or Excel", type=["csv", "xlsx"])

if uploaded is not None:
    if uploaded.name.endswith(".csv"):
        df = pd.read_csv(uploaded)
    else:
        df = pd.read_excel(uploaded)

    for idx, row in df.iterrows():
        text = str(row.get("text", ""))
        terms = parse_terms(row.get("protected_terms"))
        translate = bool(row.get("translate_embedded", False))
        res = run_pipeline(text, translate_embedded=translate, protected_terms=terms)

        st.subheader(f"Row {row.get('id', idx)}")
        col1, col2 = st.columns(2)
        with col1:
            st.text_area("Original", text, height=150, key=f"orig_{idx}")
        with col2:
            st.text_area("Clean", res['clean_text'], height=150, key=f"clean_{idx}")

        st.write("Flags:")
        if res["flags"]:
            st.table(pd.DataFrame(res["flags"]))
        else:
            st.write("None")

        st.write("Changes:")
        if res["changes"]:
            st.table(pd.DataFrame(res["changes"]))
        else:
            st.write("None")

        st.markdown("---")


--------------------------------------------------------------------------------



================================================================================
EXPORT SUMMARY
================================================================================
Total Files Exported: 36
Files Skipped: 5
Repository: C:\Users\pertt\slm-cleanroom-demo
